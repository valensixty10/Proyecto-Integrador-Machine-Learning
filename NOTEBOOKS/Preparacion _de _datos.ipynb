{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preparación de los Datos**\n",
    "La preparación de los datos es una etapa crucial antes de entrenar los modelos de machine learning. En esta fase, transformamos los datos en un formato adecuado para el modelado. Esto incluye el manejo de valores nulos, la codificación de variables categóricas, el escalado de las variables numéricas y la división de los datos en conjuntos de entrenamiento y prueba.\n",
    "\n",
    "El objetivo principal de esta fase es garantizar que los datos estén limpios, bien estructurados y listos para ser utilizados por los modelos de machine learning en la siguiente etapa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el Dataset y Crear la Carpeta de Trabajo\n",
    "Primero, nos aseguramos de que la carpeta `data` exista para almacenar los conjuntos de datos preparados. Luego, cargamos el dataset original (`ML_cars.csv`) que contiene las características y precios de los vehículos. Esta carga es necesaria para iniciar la limpieza y transformación de los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Crear la carpeta 'data' si no existe\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# 1. Cargar el dataset\n",
    "df = pd.read_csv(r\"C:/Users/Usuario/Desktop/PROYECTO_M6/Data/ML_cars.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de Valores Nulos\n",
    "Uno de los primeros pasos en la preparación de los datos es manejar los valores nulos en las columnas numéricas. Los valores nulos pueden causar problemas durante el entrenamiento del modelo, por lo que los reemplazamos con la mediana de cada columna numérica. La mediana es robusta frente a outliers y asegura que los valores sean razonables para continuar con el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Manejo de valores nulos (solo columnas numéricas)\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación de Variables Categóricas\n",
    "Muchas variables en el dataset son categóricas, como el tipo de combustible o la carrocería del vehículo. Para que los modelos de machine learning puedan procesar estas variables, utilizamos la técnica de **One-Hot Encoding**, que convierte cada categoría en una nueva columna binaria (0 o 1). Esto permite que las variables categóricas sean tratadas de manera adecuada durante el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Codificación de variables categóricas\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalado de Variables Numéricas\n",
    "El escalado de las variables numéricas es un paso importante, ya que garantiza que todas las características estén en la misma escala. Utilizamos **StandardScaler**, que transforma los valores de las variables numéricas para que tengan una media de 0 y una desviación estándar de 1. Esto es particularmente útil para modelos que son sensibles a la magnitud de las variables, como la regresión lineal o los algoritmos basados en distancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Escalado de variables numéricas\n",
    "scaler = StandardScaler()\n",
    "numeric_cols_encoded = df_encoded.select_dtypes(include=['float64', 'int64']).columns\n",
    "df_encoded[numeric_cols_encoded] = scaler.fit_transform(df_encoded[numeric_cols_encoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar las Características (X) y la Variable Objetivo (y)\n",
    "\n",
    "Dividimos el dataset en dos partes:\n",
    "- **X**: Las características del vehículo (todas las variables excepto el precio).\n",
    "- **y**: El precio del vehículo, que será la variable objetivo para predecir.\n",
    "\n",
    "Esta separación es esencial para entrenar los modelos, ya que les permitirá aprender cómo las características se relacionan con el precio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Separar las características (X) y la variable objetivo (y)\n",
    "X = df_encoded.drop('price', axis=1)\n",
    "y = df_encoded['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División en Conjuntos de Entrenamiento y Prueba\n",
    "Para evaluar el rendimiento de los modelos, dividimos los datos en un **conjunto de entrenamiento** (80%) y un **conjunto de prueba** (20%). El conjunto de entrenamiento se utiliza para entrenar el modelo, mientras que el conjunto de prueba se reserva para evaluar su rendimiento en datos no vistos, lo que nos permite medir la capacidad de generalización del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. División en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar los Conjuntos de Datos Preparados\n",
    "Una vez que los datos han sido procesados y divididos, guardamos los conjuntos de entrenamiento y prueba en archivos CSV. Esto nos permitirá reutilizar estos conjuntos en la fase de modelado, sin necesidad de volver a preparar los datos en cada ejecución. Los archivos generados se almacenan en la carpeta `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparación de datos completa.\n"
     ]
    }
   ],
   "source": [
    "# 7. Guardar los conjuntos en archivos CSV dentro de la carpeta 'data'\n",
    "X_train.to_csv('data/X_train.csv', index=False)\n",
    "X_test.to_csv('data/X_test.csv', index=False)\n",
    "y_train.to_csv('data/y_train.csv', index=False)\n",
    "y_test.to_csv('data/y_test.csv', index=False)\n",
    "\n",
    "print(\"Preparación de datos completa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusión de la Preparación de los Datos**\n",
    "\n",
    "Con la preparación de los datos completa, hemos manejado los valores nulos, transformado las variables categóricas, escalado las variables numéricas y dividido los datos en conjuntos de entrenamiento y prueba. Los datos están ahora listos para ser utilizados en la fase de modelado, donde construiremos y evaluaremos los modelos de machine learning para predecir el precio de los vehículos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Proyecto_m6_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
